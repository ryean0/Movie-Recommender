---
title: "Final Report for Movie Recommendations (Linear Model)"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The aim of this project is to develop a model to predict the ratings of movies given a specific user-movie combination, the year the movie was released and the genre of the movie.  

The report will be split into 4 sections, the __introduction__, the __analysis__, the __results__, and the __conclusion__ section.  

## Analysis

This section will involve a few key steps, namely __pre-processing__, __data visualization__, and __model development__.  

The section aims to come up with a successful model to predict the ratings of a given user-movie combination with a root-mean-squared error less than 0.85.  

__Pre-processing__

The results of the given code divide our data into 2 sets, the __validation__ set and the __edx__ set.  

In this analysis, we will use split the __edx__ set into training and test sets, while the __validation__ set will be used solely to evaluate our final algorithm.  
We start off by running code to import our libraries and generate our datasets from the movielens database.  
We use the semi_join function to ensure that our user-movie combinations are the same in both the edx and validation sets.  

```{r eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
# installing required libraries
library(tidyverse)
library(dslabs)
library(tidytext)
library(dplyr)
library(caret)
library(matrixStats)
library(data.table)
library(stringr)
library(lubridate)
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")


movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1)


test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
# splits edx dataset into training and test sets
options(digits=5)
set.seed(1)
val <- validation
t_ind <- createDataPartition(y=edx$rating, times=1, p=0.05, list=FALSE)
train_x <- edx[-t_ind, ]
test_x <- edx[t_ind, ]
```
When we view the train_x dataset, we observe 2 things,    
1. The year each movie is released is in the form of the number of days starting from 1 Jan 1970.  
2. The genres of each movie fall into many different categories, making it difficult to sort the data.  

Our task is then to process this data into a more usable and convertible form. The train_x dataset is printed below.  

```{r eval=TRUE, echo= TRUE}
train_x
```
In order to analyse the variation of ratings across time (year), we first need to process the data and convert the timestamp column to years.  
To do so, we use the __as_datetime__ function in the __lubridate__ package.    
Code is as follows:  

``` {r eval=TRUE, echo=TRUE, tidy=TRUE}
# convert timestamp column to year
train_x$timestamp <- as_datetime(train_x$timestamp) %>% format('%Y')
train_x$timestamp <- as.numeric(train_x$timestamp)
test_x$timestamp <- as_datetime(test_x$timestamp) %>% format('%Y')
test_x$timestamp <- as.numeric(test_x$timestamp)
val$timestamp <- as_datetime(val$timestamp) %>% format('%Y')
val$timestamp <- as.numeric(val$timestamp)
```
For the genres, we will be analysing them by grouping combinations of genres with more than 500 ratings together.  
This will be done in a future section.

__Data Visualization__

To decide which factors we should consider in our model, we first need to explore their variation amongst the dataset.  

The plots show the variation in number of ratings movies were given and the variation in how many ratings different users gave.    
```{r eval=TRUE, echo=FALSE}
train_x %>% 
     dplyr::count(movieId) %>% 
     ggplot(aes(n)) + 
     geom_histogram(bins = 30, color = "black") + 
     scale_x_log10() + 
     ggtitle("Movies")

train_x %>%
     dplyr::count(userId) %>% 
     ggplot(aes(n)) + 
     geom_histogram(bins = 30, color = "black") + 
     scale_x_log10() +
     ggtitle("Users")
```
There is significant variation amongst the number of ratings given for movies, users and year.  

This suggests the following trends.  
1. Different movies could have different inherent qualities that result in different number of ratings, ie. blockbuster films vs unknown ones  

2. Different users have different levels of activity on the rating site  

3. Different years might have garnered more or less ratings due to number of movies released in the year, or external factors which affect ratings  

4. We can also assume that the genre of movies has a direct effect on its ratings, since action movies might be given higher ratings simply due to more people rating them  


__Model Development__

Now that we have 4 different factors affecting our final rating, we can define our linear model as such:  

__rating(Y)__ = __mean(u)__ + __b_i (movie effect)__ + __b_u (user effect)__ + __b_t (year effect)__ + __b_g (genre effect)__

The end goal would be to derive a set of these 4 variables for each user-movie combination, allowing us to estimate the rating.  

The next step of our model development would be to define a function for our root-mean-squared error (RMSE). Code is as follows:

```{r eval=TRUE, echo=TRUE }

RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}

```
Before we begin generating the 4 effects for our model, we can use regularization to improve our data.  

Since some large estimates of ratings can come from users who have rated very few movies, or from really obscure movies, we can introduce a penalty term lambda to shrink the values of the effects if their sample size is small.  

In order to determine the optimum value of lambda, we can perform cross validation to choose the lambda which minimises the RMSE on the training set.  

The code is not run here, but can be referenced from the R Script. We end up with an optimum lambda value of 4.8.    

We can now begin to develop our model effects and append each effect to the training set.  

For genre effects, since there are many different combinations, we only choose combinations to group the data by if the number of that specific combination is more than 500.  

Since the combinations that have less than 500 entries can be imagined as more obscure, we will replace the NA values on their gender effect with 0s.  

We will also replace all effects which have NA values with 0, since if the effects are not evaluated, they can be assumed to be insignificant.  

The final training set and the code to generate the model is shown below.  


``` {r eval=TRUE, echo=TRUE, tidy=TRUE, message=FALSE}
l <- 4.8
mu_hat <- mean(edx$rating)

#modelling movie effects
movie_avgs <- train_x %>% group_by(movieId) %>% summarize(b_i=sum(rating-mu_hat)/(n()+l))
train_x <- train_x %>% left_join(movie_avgs, by='movieId')

#modelling user effects
user_avgs <- train_x %>% group_by(userId) %>% summarize(count=n(), b_u=sum(rating-mu_hat-b_i)/(n()+l))
user_avgs <- user_avgs[user_avgs$count>10,]
train_x <- train_x %>% left_join(user_avgs, by='userId')
train_x$b_u[is.na(train_x$b_u)] <- 0

#modelling year effects
year_avgs <- train_x %>% group_by(timestamp) %>% summarize(b_t=sum(rating-mu_hat-b_i-b_u)/(n()+l))
train_x <- train_x %>% left_join(year_avgs, by='timestamp')
train_x$b_t[is.na(train_x$b_t)] <- 0

#modelling genre effects
genre_avgs <- train_x %>% group_by(genres) %>% summarize(count=n(), b_g=sum(rating-mu_hat-b_i-b_u-b_t)/(n()+l))
genre_avgs <- genre_avgs[genre_avgs$count > 500,]
train_x <- train_x %>% left_join(genre_avgs, by='genres')
train_x$b_g[is.na(train_x$b_g)] <- 0

head(train_x)
```
Next we will generate our predicted ratings and save it to the variable __pred__ in the __predicted_ratings__ variable.  

The model is first evaluated on the test_x set, and then the final RMSE is computed on the validation set, __val__.  

```{r eval=TRUE, echo=FALSE, message=FALSE}
# predict ratings on test set
predicted_ratings <- test_x %>%
left_join(movie_avgs, by='movieId') %>%
left_join(user_avgs, by='userId') %>%
left_join(year_avgs, by='timestamp') %>%
  left_join(genre_avgs, by='genres') 

predicted_ratings$b_i[is.na(predicted_ratings$b_i)] <- 0
predicted_ratings$b_u[is.na(predicted_ratings$b_u)] <- 0
predicted_ratings$b_t[is.na(predicted_ratings$b_t)] <- 0
predicted_ratings$b_g[is.na(predicted_ratings$b_g)] <- 0

predicted_ratings <- predicted_ratings %>% mutate(pred= mu_hat + b_i+ b_u + b_t + b_g) 
```
Lastly, we replace all predicted ratings that have NA values with a value slightly below the mean. In this case we choose it to be 0.1 below the mean rating.  

The final RMSE is computed and returned as follows:

```{r eval=TRUE, echo=TRUE}
predicted_ratings$pred[is.na(predicted_ratings$pred)] <- mu_hat - 0.1
```
```{r eval=TRUE, echo=TRUE}
RMSE(test_x$rating, predicted_ratings$pred)
```
## Results

To test our final model and get the RMSE, we compute predictions based on our validation set.  

```{r eval=TRUE, echo=FALSE}
predicted_ratings <- val %>%
left_join(movie_avgs, by='movieId') %>%
left_join(user_avgs, by='userId') %>%
left_join(year_avgs, by='timestamp') %>%
  left_join(genre_avgs, by='genres') 

predicted_ratings$b_i[is.na(predicted_ratings$b_i)] <- 0
predicted_ratings$b_u[is.na(predicted_ratings$b_u)] <- 0
predicted_ratings$b_t[is.na(predicted_ratings$b_t)] <- 0
predicted_ratings$b_g[is.na(predicted_ratings$b_g)] <- 0

predicted_ratings <- predicted_ratings %>% mutate(pred= mu_hat + b_i+ b_u + b_t + b_g) 
predicted_ratings$pred[is.na(predicted_ratings$pred)] <- mu_hat - 0.1
```
```{r eval=TRUE, echo=TRUE}
RMSE(val$rating, predicted_ratings$pred)
```

As we can see, we get a final RMSE of 0.86487, which achieves our goal of getting a RMSE value of less than 0.85.  


## Conclusion

The use of 4 different factors to model the variation of ratings across user-movie combinations helps to reduce discrepancies and provide a more realistic modelling of our data. However, the linear model used for this statistical modelling is seldom effective when considering real-world data which tends to __not__ follow a linear model.  

Furthermore, to fit a linear model, a better way would be to use the __lm__ function in the caret package. However, due to the limits of our computational capacity, fitting such a model becomes impractical at our level. The current method of estimating effects should suffice.  

To extend this model, we could consider implementing matrix factorization to analyze the user-movie variation for each user-movie combination. We would calculate the residuals and implement singular value decomposition to get the principal components of our decomposition. This could further decrease our RMSE as it accounts for user-movie variation which we did not consider in our current model.  
